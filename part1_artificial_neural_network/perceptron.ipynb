{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning with Python\n",
    "\n",
    "Welcome to the **Deep Learning** course! This course is designed to give you hands-on experience with the foundational concepts and advanced techniques in deep learning. You will explore:\n",
    "\n",
    "- Artificial Neural Networks and Gradient Descent\n",
    "- Convolutional Neural Networks (CNNs) for Computer Vision\n",
    "- Recurrent Neural Networks (RNNs) for Text Prediction\n",
    "- Diffusion Transformers for Image Generation\n",
    "\n",
    "Throughout the course, you'll engage in projects to solidify your understanding and gain practical skills in implementing deep learning algorithms.  \n",
    "\n",
    "Instructor: Dr. Adrien Dorise  \n",
    "Contact: adrien.dorise@hotmail.com  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Perceptron with Gradient Descent\n",
    "In this project, you will build a perceptron from scratch and gain a deeper understanding of how gradient descent works in optimizing models. The tasks you will complete include:\n",
    "\n",
    "1. **Import and Understand a Dataset**: Learn how to load, preprocess, and explore a dataset to prepare it for training.\n",
    "2. **Implement a Perceptron from Scratch**: Code the perceptron algorithm using only fundamental Python and NumPy.\n",
    "3. **Implement Gradient Descent**: Incorporate gradient descent to optimize the perceptron's weights.\n",
    "4. **Evaluate the Model on Simple Logical Functions**: Test your perceptron on logical functions like AND, OR, and XOR to understand their capabilities and limitations.\n",
    "5. **Evaluate the Model on a Real-World Dataset**: Apply your perceptron to a simple real-world dataset and assess its performance.\n",
    "\n",
    "By the end of this project, you'll have a solid understanding of how perceptrons work and how gradient descent improves their performance. This foundational knowledge will prepare you for more advanced neural network architectures in subsequent projects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This exercise will start by importing a simple dataset based on AND, OR and XOR logical functions.  \n",
    "These functions consist of binary inputs and outputs, used for training and testing basic neural network models. \n",
    "\n",
    "**AND Dataset**  \n",
    "The AND operation returns 1 if both inputs are 1, and 0 otherwise.\n",
    "\n",
    "| Input 1 | Input 2 | Output (AND) |\n",
    "|---------|---------|--------------|\n",
    "| 0       | 0       | 0            |\n",
    "| 0       | 1       | 0            |\n",
    "| 1       | 0       | 0            |\n",
    "| 1       | 1       | 1            |\n",
    "\n",
    "**OR Dataset**  \n",
    "The OR operation returns 1 if at least one input is 1.\n",
    "\n",
    "| Input 1 | Input 2 | Output (OR) |\n",
    "|---------|---------|-------------|\n",
    "| 0       | 0       | 0           |\n",
    "| 0       | 1       | 1           |\n",
    "| 1       | 0       | 1           |\n",
    "| 1       | 1       | 1           |\n",
    "\n",
    "**XOR Dataset**  \n",
    "The XOR operation returns 1 if the inputs are different, and 0 if they are the same.\n",
    "\n",
    "| Input 1 | Input 2 | Output (XOR) |\n",
    "|---------|---------|--------------|\n",
    "| 0       | 0       | 0            |\n",
    "| 0       | 1       | 1            |\n",
    "| 1       | 0       | 1            |\n",
    "| 1       | 1       | 0            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## AND dataset\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "## AND dataset\n",
    "and_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "and_outputs = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "## OR dataset\n",
    "or_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "or_outputs = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "## XOR dataset\n",
    "xor_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_outputs = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "\n",
    "# Explore the dataset\n",
    "for i in range(len(and_inputs)):\n",
    "    print(f\"{and_inputs[i]} -> {and_outputs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Perceptron\n",
    "\n",
    "<img src=\"../docs/perceptron.jpg\" alt=\"Perceptron\" width=\"500\"/>  \n",
    "\n",
    "\n",
    "\n",
    "You will now have to implement the perceptron from scratch!\n",
    "It means recreating its function:  \n",
    "$\n",
    "\\hat{y} = f(\\sum_{i=1}^{n}(w_i x_i) + b) = f(w_1x_1 + w_2x_2 +...+w_nx_n +b)\n",
    "$\n",
    "\n",
    "It is done by:\n",
    " - Implementing the weighted sum\n",
    " - Implementing the activation function\n",
    " - Creating the perceptron\n",
    " - Defining the loss function\n",
    " - Implementing the optimisation with gradient descent\n",
    "    - Weights update\n",
    "    - Bias update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted sum**\n",
    "- Complete the code below to create the weighted sum function.\n",
    "- It has to take an input vector, a weight vector and a bias value as input. \n",
    "- Feel free to experiment with the dataset created before to be sure that your function is working as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "weights = None\n",
    "bias = None\n",
    "\n",
    "def weighted_sum(x, weights, bias):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation function**\n",
    "- Complete the code below to create the activation function\n",
    "- You can choose whatever activation function you want. \n",
    "- Be careful, as the step function has no gradient.\n",
    "- Sigmoid or ReLu functions seem to be a good fit!   \n",
    "$\n",
    "ReLU(x) = \n",
    "\\begin{cases} \n",
    "\tw, & \\text{if } x > 0 \\\\\n",
    "\t0, & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$  \n",
    "$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def activation_function(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron**\n",
    "- Complete the code below to create the forward function of your percetron.\n",
    "- feel free to test with the dummy data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = [2, 1, 0, -1]\n",
      "bias = 0.5\n",
      "inputs = [1, 2, 3, 4]\n",
      "weighted sum = None\n",
      "prediction = None\n"
     ]
    }
   ],
   "source": [
    "dummy_x = [1,2,3,4]\n",
    "dummy_weights = [2,1,0,-1]\n",
    "dummy_bias = 0.5\n",
    "\n",
    "def predict(x, weights, bias):\n",
    "    \n",
    "    #TODO \n",
    "    \n",
    "    pass\n",
    "\n",
    "print(f\"weights = {dummy_weights}\")\n",
    "print(f\"bias = {dummy_bias}\")\n",
    "print(f\"inputs = {dummy_x}\")\n",
    "print(f\"weighted sum = {weighted_sum(dummy_x, dummy_weights, dummy_bias)}\")   \n",
    "print(f\"prediction = {predict(dummy_x, dummy_weights, dummy_bias)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function** \n",
    "- Complete the code below to create your custom loss function.\n",
    "- You will have to make a function that calculates the loss of a single sample, and a function that takes a batch of samples as input\n",
    "- The L2 loss is a good start:  \n",
    "$\n",
    "\\mathcal{L}(\\hat{y},y) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def compute_loss_sample(prediction, target):\n",
    "    pass\n",
    "\n",
    "def compute_loss_batch(predictionq, targets):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient**  \n",
    "The gradient update is a formula of the form:  \n",
    "$ \\theta_i \\leftarrow \\theta_i - \\alpha \\nabla_{\\theta_i} J(\\theta_i) $\n",
    "\n",
    "- Complete the code below to compute the gradient of the loss function $\\nabla_{\\theta_i} \\mathcal{L}_{\\theta_i}$ \n",
    "    - in regards to the weights\n",
    "    - in regards to the bias\n",
    "- You will have to return the gradient for each weight in your perceptron as a vector.\n",
    "- **Be careful to include the derivative of your activation function!**\n",
    "  - Create a function to compute the derivative in regards to the weighted some *z*.\n",
    "- If you are lost, go back to part 1 of the course.\n",
    "    - Get back to the slides 38->48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'and_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mand_inputs\u001b[49m\n\u001b[1;32m      4\u001b[0m targets \u001b[38;5;241m=\u001b[39m and_outputs\n\u001b[1;32m      5\u001b[0m weights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'and_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "inputs = and_inputs\n",
    "targets = and_outputs\n",
    "weights = [2,-1]\n",
    "bias = 0.5\n",
    "\n",
    "def derivative(z):\n",
    "    pass\n",
    "\n",
    "def gradient_weights(inputs, targets, weights, bias):\n",
    "    dw = np.zeros(len(inputs[0]))\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return dw\n",
    "\n",
    "def gradient_bias(targets, weights, bias):\n",
    "    db = 0\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "Now that you have computed the gradient, you can perform the gradient descent!\n",
    "- You have to perform the full parameters updates using what you created.\n",
    "    - Get data from the dataset\n",
    "    - Predict new value with your model\n",
    "    - Compute the gradient\n",
    "    - Update the parameters\n",
    "    - Return the final parameters\n",
    "    - *Compute the loss of the epoch, to verify that the model is learning*\n",
    "        - Return a list of the losses for each epoch\n",
    "- Recall the gradient update function if needed\n",
    "    - Make use of the learning parameter $\\alpha$.\n",
    "- The gradient descent is an iterative algorithm\n",
    "    - Loop the algorithm until you reach a set number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def fit(inputs, targets, weights, bias, alpha, max_iter):\n",
    "    losses =  []\n",
    "    pass\n",
    "\n",
    "inputs = and_inputs\n",
    "targets = and_outputs\n",
    "weights = [2,-1]\n",
    "bias = 0.5\n",
    "alpha = 0.05\n",
    "n_epoch = 500\n",
    "w,b,l = fit(inputs, targets, weights, bias, alpha, n_epoch)\n",
    "print(f\"weights: {w} / bias: {b} / loss: {l[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot results**\n",
    "\n",
    "Now that your model is working fine, you can plot the result to verify the training\n",
    "- Compute the losses for the three dataset AND, OR and XOR.\n",
    "- Use matplotlib to plot the three losses on a single figure.\n",
    "- Conclude on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(loss_and, loss_or, loss_xor):\n",
    "    plt.plot(loss_and, label='AND')\n",
    "    plt.plot(loss_or, label='OR')\n",
    "    plt.plot(loss_xor, label='XOR')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Perceptron training\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# TODO\n",
    "\n",
    "loss_and = None\n",
    "loss_or = None\n",
    "loss_xor = None\n",
    "plot_losses(loss_and, loss_or, loss_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS\n",
    "\n",
    "Now that you have a perfect understanding of the perceptron and its limitations, you can try your algorithm on real-world datasets!\n",
    "Below are some examples that you can get from sklearn or the *Kaggle* website.\n",
    "\n",
    "**Iris Dataset (Binary Subset)**  \n",
    "Description: The Iris dataset is a classic dataset in machine learning. You can use only two classes (e.g., Setosa and Versicolor) and two features (e.g., sepal length and width) to make it linearly separable.\n",
    "\n",
    "**Breast Cancer Dataset**  \n",
    "Description: A dataset for binary classification, differentiating between malignant and benign tumors.\n",
    "\n",
    "**Titanic Dataset**  \n",
    "Description: A dataset to predict survival on the Titanic. Simplify it by selecting features like Pclass, Age, and Sex and treat survival as the target.\n",
    "\n",
    "**Pima Indians Diabetes Dataset**  \n",
    "Description: A dataset to predict the onset of diabetes based on diagnostic measures like glucose concentration, blood pressure, and BMI.\n",
    "\n",
    "*Kaggle is your go-to community for data science. It has tons of public datasets and challenges to help you improve.*\n",
    "\n",
    "**Your job**\n",
    "- Train your perceptron on one of these datasets.\n",
    "- Try to get the best possible model by varying the hyperparameters\n",
    "- Analyse if the perceptron is limited due to its non-linearity (try to plot the data, or compare with non-linear models if you know some)\n",
    "- Conclude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Utility Function: Scale Features\n",
    "def scale_features(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled\n",
    "\n",
    "# Iris Dataset (Binary Subset)\n",
    "def load_iris_dataset():\n",
    "    iris = load_iris()\n",
    "    mask = iris.target != 2  # Exclude the third class (Virginica)\n",
    "    X = iris.data[mask, :2]  # Only first two features\n",
    "    y = iris.target[mask]\n",
    "    return X, y\n",
    "\n",
    "# Breast Cancer Dataset\n",
    "def load_breast_cancer_dataset():\n",
    "    data = load_breast_cancer()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    return X, y\n",
    "\n",
    "# Titanic Dataset\n",
    "def load_titanic_dataset():\n",
    "    url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "    # Select features: Pclass, Age, Sex\n",
    "    df = df[['Pclass', 'Age', 'Sex', 'Survived']].dropna()\n",
    "    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Convert categorical to numeric\n",
    "    X = df[['Pclass', 'Age', 'Sex']].values\n",
    "    y = df['Survived'].values\n",
    "    return X, y\n",
    "\n",
    "# Pima Indians Diabetes Dataset\n",
    "def load_pima_diabetes_dataset():\n",
    "    url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "    columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', \n",
    "               'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "    dataset = pd.read_csv(url, names=columns)\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset['Outcome'].values\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "\n",
    "# Iris dataset\n",
    "X, y = load_iris_dataset()\n",
    "X = scale_features(X)\n",
    "print(\"\\nIris dataset loaded and scaled.\")\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(f\"First samples: {X[0]} -> {y[0]}\")\n",
    "\n",
    "# Breast Cancer dataset\n",
    "X, y = load_breast_cancer_dataset()\n",
    "X = scale_features(X)\n",
    "print(\"\\nBreast Cancer dataset loaded and scaled.\")\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(f\"First samples: {X[0]} -> {y[0]}\")\n",
    "\n",
    "# Titanic dataset\n",
    "X, y = load_titanic_dataset()\n",
    "X = scale_features(X)\n",
    "print(\"\\nTitanic dataset loaded and scaled.\")\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(f\"First samples: {X[0]} -> {y[0]}\")\n",
    "\n",
    "# Pima Diabetes dataset\n",
    "X, y = load_pima_diabetes_dataset()\n",
    "X = scale_features(X)\n",
    "print(\"\\nPima Diabetes dataset loaded and scaled.\")\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(f\"First samples: {X[0]} -> {y[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
