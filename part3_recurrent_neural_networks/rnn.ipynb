{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "\n",
    "Welcome to the **Deep Learning** course! This course is designed to give you hands-on experience with the foundational concepts and advanced techniques in deep learning. You will explore:\n",
    "\n",
    "- Artificial Neural Networks and Gradient Descent\n",
    "- Convolutional Neural Networks (CNNs) for Computer Vision\n",
    "- Recurrent Neural Networks (RNNs) for Text Prediction\n",
    "- Diffusion Transformers for Image Generation\n",
    "\n",
    "Throughout the course, you'll engage in projects to solidify your understanding and gain practical skills in implementing deep learning algorithms.  \n",
    "\n",
    "Instructor: Dr. Adrien Dorise  \n",
    "Contact: adrien.dorise@hotmail.com  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Recurent Neural Networks for text prediction\n",
    "\n",
    "In this project, you will build a language model using Long Short-Term Memory (LSTM) networks in PyTorch. This exercise will guide you through the essential steps:  \n",
    "\n",
    "**1. Data Management**  \n",
    "\n",
    "- **Objective**: Divide the dataset into training, validation, and test sets to ensure unbiased evaluation.\n",
    "- **Steps**:\n",
    "   - Loading and preprocessing a text dataset  \n",
    "   - Preparing the dataset for sequential modelling  \n",
    "   - Splitting data into training and testing sets  \n",
    "\n",
    "**2. Tokenization**\n",
    "\n",
    "- **Objective**: Transform text data into numerical values to be fed to a neural network.\n",
    "- **Steps**:\n",
    "   - Converting text into numerical representations using tokenization  \n",
    "   - Padding sequences for uniform input size  \n",
    "   - Creating word embeddings for efficient learning  \n",
    "\n",
    "**3. Building an LSTM Model with PyTorch**  \n",
    "\n",
    "- **Objective**: Build a language model based on LSTM architecture.\n",
    "- **Steps**:\n",
    "   - Defining an LSTM architecture  \n",
    "   - Initializing model parameters  \n",
    "   - Training the model with an appropriate loss function and optimizer  \n",
    "\n",
    "**4. Prediction and Evaluation**  \n",
    "\n",
    "- **Objective**: Evaluate the model performance on text generation\n",
    "- **Steps**:\n",
    "   - Generating text predictions using the trained model  \n",
    "   - Evaluating model performance using metrics like perplexity  \n",
    "   - Fine-tuning hyperparameters for better results  \n",
    "\n",
    "\n",
    "By completing this exercise, you will gain hands-on experience in implementing LSTMs for text prediction while understanding key challenges like sequence dependency, vanishing gradients, and text generation strategies.\n",
    "\n",
    "*Credits*: This project is inspired by the *Text generation with LSTM in Pytorch* article on Machine Learning Mastery by Jason Brownlee, PHD.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "\n",
    "In this exercise, we will take the data from **Alice in Wonderland** by Lewis Caroll  \n",
    "The data can be found in the text file *alice_in_wonderland.txt* file in the same folder.  \n",
    "\n",
    "The book was downloaded from this website:\n",
    "`https://www.gutenberg.org/ebooks/11`\n",
    "\n",
    "To simplify the prediction, the entire dataset is set to lowercase.\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the txt file into a Python variable\n",
    "\n",
    "filename = \"alice_in_wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower() #Convert to lower case\n",
    "#print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "As you may have noticed, neural networks take input values and predict output values.\n",
    "The keyword here is value. However, when we create a language model (for translation, correction…), we do not take values as input, but text.  \n",
    "\n",
    "In order to take text as input, we have to translate it into a neural network friendly format: scalars.  \n",
    "This process is called tokenisation.\n",
    "\n",
    "Three main methods are used in tokenisation:\n",
    "- Character based tokenisation\n",
    "- Word based tokenisation\n",
    "- Subword based tokenisation\n",
    "\n",
    "<img src=\"../docs/tokenisation.jpg\" alt=\"tokenisation\" width=\"800\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet below is a class **Tokenizer** that implements all three tokenisation methods.  \n",
    "In the snippet below, the `Tokenizer` class tokenizes text using Character, Word, or Subword (BPE) methods. It builds a vocabulary from the input text and supports encoding and decoding. \n",
    "\n",
    "### Methods\n",
    "- `fit(text)`: Builds vocabulary from input text. It is called at the initialisation  of the class.\n",
    "- `encode(text)`: Converts text into tokenized representation.\n",
    "- `decode(tokens)`: Converts tokens back into text.\n",
    "\n",
    "### Your job:\n",
    "  - A short example using this class is given after \n",
    "  - Test all three methods and explain the differences in token lengths and total tokens needed to map the dataset.\n",
    "  - You can glimpse the vocabulary used during tokenisation with the *vocab* variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import tokenizers as t\n",
    "import re \n",
    "\n",
    "class Tokenisation_method(Enum):\n",
    "    CHARACTER = 1\n",
    "    WORD = 2\n",
    "    SUBWORD = 3\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, input_text, tokenisation_method=Tokenisation_method.CHARACTER):\n",
    "        self.method = tokenisation_method\n",
    "        self.input_text = input_text.lower()\n",
    "        self.vocab = self.fit(self.input_text)\n",
    "        self.n_vocab = len(self.vocab)\n",
    "\n",
    "    def fit(self, text):\n",
    "            \"\"\"Builds a vocabulary from the given text.\"\"\"\n",
    "\n",
    "            if self.method is Tokenisation_method.CHARACTER:\n",
    "                # Create a vocabulary mapping each unique character to a unique integer.\n",
    "                # We add -1 at idx to account the fact that dict starts at 1 and not 0\n",
    "                vocab = {char: idx-1 for idx, char in enumerate(sorted(set(text)), start=1)}\n",
    "            \n",
    "            elif self.method is Tokenisation_method.WORD:\n",
    "                # Create a vocabulary mapping each unique word to a unique integer\n",
    "                # We add -1 at idx to account the fact that dict starts at 1 and not 0\n",
    "                \n",
    "                # Tokenize while preserving '\\n' as a separate token\n",
    "                tokens = re.findall(r'\\S+|\\n', text)  # Matches words and newline characters\n",
    "                vocab = {word: idx - 1 for idx, word in enumerate(sorted(set(tokens)), start=1)}\n",
    "            \n",
    "            elif self.method is Tokenisation_method.SUBWORD:\n",
    "                # Subword-based tokenization using Byte Pair Encoding (BPE)\n",
    "                  \n",
    "                if isinstance(text, str):\n",
    "                    text = [text]  # Convert to a list\n",
    "\n",
    "                # Initialise BPE tokenizer\n",
    "                self.subword_tokenizer = t.Tokenizer(t.models.BPE(unk_token=\"<UNK>\"))\n",
    "                self.subword_tokenizer.pre_tokenizer = t.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "                self.subword_tokenizer.decoder = t.decoders.ByteLevel()\n",
    "\n",
    "                # Train the tokenizer on provided text\n",
    "                trainer = t.trainers.BpeTrainer(vocab_size=2000,special_tokens=[\"<UNK>\"])\n",
    "                self.subword_tokenizer.train_from_iterator(text, trainer=trainer)\n",
    "                vocab = self.subword_tokenizer.get_vocab()\n",
    "\n",
    "            return vocab\n",
    "    \n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Tokenize an input text based on the vocabulary learned when initialising the tokenizer.\"\"\"\n",
    "\n",
    "        if self.method is Tokenisation_method.CHARACTER:\n",
    "            return [self.vocab[char] for char in text if char in self.vocab]\n",
    "        \n",
    "        elif self.method is Tokenisation_method.WORD:\n",
    "            words = re.findall(r'\\S+|\\n', text)\n",
    "            return [self.vocab[w] for w in words if w in self.vocab]\n",
    "        \n",
    "        elif self.method is Tokenisation_method.SUBWORD:\n",
    "            return self.subword_tokenizer.encode(text).ids\n",
    "            \n",
    "        \n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Convert a list of integer tokens back into text.\"\"\"\n",
    "\n",
    "        if self.method is Tokenisation_method.CHARACTER:\n",
    "            inv_vocab = {idx: char for char, idx in self.vocab.items()}\n",
    "            return ''.join(inv_vocab[token] for token in tokens if token in inv_vocab)\n",
    "        \n",
    "        elif self.method is Tokenisation_method.WORD:\n",
    "            inv_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "            return ' '.join(inv_vocab[token] for token in tokens if token in inv_vocab)\n",
    "        \n",
    "        elif self.method is Tokenisation_method.SUBWORD:\n",
    "            return self.subword_tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "tokenizer = \n",
    "input = \n",
    "tokens = \n",
    "\n",
    "print(f\"Input text: {input}\")\n",
    "print(f\"Encoded tokens: {tokens}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(tokens)}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.vocab)}\")\n",
    "print(f\"Vocabulary: {tokenizer.vocab}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "In order to train an LSTM model for text prediction, the raw tokenised data must be preprocessed. This involves normalising the tokens, converting them into sequences, and preparing them as PyTorch tensors. \n",
    "Indeed, compared to non-sequential architectures, each sample of a LSTM is a sequence of multiple tokens.\n",
    "\n",
    "In the code snippet below, the `Transform_Tokens` class is designed to handle these preprocessing steps. It includes methods to scale tokens using MinMax scaling, transform token sequences into training pairs, and convert them into PyTorch tensors.  \n",
    "\n",
    "The key steps in this process are:  \n",
    "- **Scaling the tokens:** The MinMaxScaler maps tokens to a range between 0 and 1 for better neural network performance.  \n",
    "- **Creating input sequences:** The raw token list is converted into overlapping sequences of a fixed length, with the next token as the target.  \n",
    "- **Converting sequences to tensors:** The sequences are reshaped to match the expected input format for PyTorch models.  \n",
    "\n",
    "### Your job:\n",
    "  - Transform the tokens into a valid LSTM input using Transform_Tokens.\n",
    "  - Analyse the inputs and outputs of the tensor.\n",
    "  - Print a few samples with their targets as tensor values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "class Transform_Tokens:\n",
    "    def __init__(self, non_scaled_tokens):\n",
    "        self.scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        self.scaler.fit(np.array(non_scaled_tokens).reshape(-1, 1))\n",
    "\n",
    "    def transform_tokens(self,tokens, sequence_length, with_target=True):\n",
    "        features, targets = self.token_to_sequence(tokens,sequence_length, with_target)\n",
    "        features, targets = self.sequence_to_torch(features, targets)\n",
    "        return features, targets\n",
    "    \n",
    "    def scale_tokens(self, unscaled_tokens):\n",
    "        data = np.array(unscaled_tokens).reshape(-1, 1)\n",
    "        return self.scaler.transform(data).flatten()\n",
    "\n",
    "    def unscale_tokens(self, scaled_tokens):\n",
    "        data = np.asarray(scaled_tokens).reshape(-1, 1)\n",
    "        unscaled = self.scaler.inverse_transform(data).flatten()\n",
    "        unscaled = [math.ceil(token) for token in unscaled]\n",
    "        return unscaled\n",
    "\n",
    "    def token_to_sequence(self,tokens, sequence_length=100, with_target=True):\n",
    "        \"\"\"prepare the dataset of input to output pairs and normalise the features\"\"\"\n",
    "        features = []\n",
    "        targets = []\n",
    "        scaled_tokens = self.scale_tokens(tokens)\n",
    "        num_sequences = len(tokens) - sequence_length\n",
    "        if not with_target:\n",
    "            num_sequences += 1\n",
    "        for i in range(num_sequences):\n",
    "            seq_in = scaled_tokens[i:i + sequence_length]\n",
    "            features.append([tok for tok in seq_in])\n",
    "            if with_target:\n",
    "                seq_out = tokens[i + sequence_length]\n",
    "                targets.append(seq_out)\n",
    "        return features, targets\n",
    "\n",
    "    def sequence_to_torch(self, sequences, targets,):\n",
    "        # reshape X to be [batch size, time steps, features]\n",
    "        seq_length = len(sequences[0])\n",
    "        sequences = torch.tensor(sequences, dtype=torch.float32).reshape(len(sequences), seq_length, 1)\n",
    "        targets = torch.tensor(targets)\n",
    "        return sequences, targets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "sequence_length = \n",
    "transform = \n",
    "\n",
    "print(f\"Input tokens: {tokens}\\n\")\n",
    "\n",
    "features, targets = \n",
    "\n",
    "print(f\"Scaled torch Feature -> Unscaled Torch Target\")\n",
    "for i in range(5):\n",
    "    print(f\"{features[i].flatten()} -> {targets[i]}\")\n",
    "\n",
    "print(f\"\\nUnscaled Feature -> Unscaled Target\")\n",
    "for i in range(5):\n",
    "    unscaled_features = transform.unscale_tokens(features[i])\n",
    "    print(f\"{unscaled_features} -> {targets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "Now that we have the dataset nice and running, we can focus on creating the LSTM model as well as the training function.  \n",
    "\n",
    "Below, the `LanguageModel` class implements an LSTM-based neural network for text prediction. This model takes sequences of tokenized text as input and predicts the next token. Similarly to CNN, you have to create your own torch Module and implement the different layers of your model, as well as the forward function.  \n",
    "\n",
    "**Your job:**\n",
    "- Complete the `LanguageModel` class below to create a LSTM model. Here are the tips for the architecture:\n",
    "    - **LSTM layers:** The model consists of LSTM layers. LSTMs are effective for handling sequential data because they maintain long-term dependencies.  \n",
    "    - **Dropout layer:** A dropout layer to help prevent overfitting by randomly dropping connections during training.  \n",
    "    - **Linear layer:** A fully connected layer to map the LSTM outputs to the vocabulary size, producing logits for token prediction.  \n",
    "    - **Activation function:** The ReLU activation function introduces non-linearity in the output.  \n",
    "\n",
    "#### Forward Pass  \n",
    "- The input passes through the LSTM layers.  \n",
    "- Only the last output from the sequence is taken to make a prediction.  \n",
    "- The dropout layer is applied to prevent overfitting.  \n",
    "- The final output is produced through the linear layer and activated using an activation function (you can start with ReLU).  \n",
    "\n",
    "*Note*: I want you to focus particularly on the output of the linear layer. \n",
    "- What is its output size? \n",
    "- What does it mean about the way we are predicting tokens?\n",
    "- Don't hesitate to ask your professor if you have any doubt, I am here for that!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    " \n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab, sequence_length):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(256, n_vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` function is responsible for training the LSTM model using the Adam optimizer and cross-entropy loss. It processes the dataset in batches and performs training for a specified number of epochs.\n",
    "In short, this training function takes care of getting prediction out of your model and implements **backpropagation through time**.\n",
    "\n",
    "#### Training Steps  \n",
    "1. **Data Preparation:**  \n",
    "   - The dataset is wrapped in a PyTorch `DataLoader`, which enables batch processing and shuffling.  \n",
    "   - The loss function used is `CrossEntropyLoss`, which is standard for multi-class classification tasks.  \n",
    "\n",
    "2. **Training Loop:**  \n",
    "   - The model is set to training mode.  \n",
    "   - For each batch:\n",
    "     - Predictions are made using the forward pass.  \n",
    "     - The loss between predictions and actual targets is calculated.  \n",
    "     - Gradients are computed using backpropagation.  \n",
    "     - The optimizer updates the model’s parameters.  \n",
    "\n",
    "3. **Validation Step:**  \n",
    "   - The model is switched to evaluation mode (`model.eval()`).  \n",
    "   - The loss is computed without updating gradients.  \n",
    "   - The best model (with the lowest validation loss) is saved.  \n",
    "\n",
    "4. **Model Checkpointing:**  \n",
    "   - The best-performing model state is saved as `\"language_model.pth\"`, ensuring the best version is available for future use.  \n",
    "\n",
    "This function ensures that the LSTM model is efficiently trained and evaluated, leading to optimal performance on text prediction tasks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fit(train_set, val_set, n_epochs, batch_size, model, device):\n",
    "    optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    \n",
    "    feat_train, targ_train = train_set\n",
    "    feat_val, targ_val = val_set\n",
    "    loader_train = data.DataLoader(data.TensorDataset(feat_train, targ_train), shuffle=True, batch_size=batch_size)\n",
    "    loader_val = data.DataLoader(data.TensorDataset(feat_val, targ_val), shuffle=False, batch_size=batch_size)\n",
    "    losses = [[],[]]\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in loader_train:\n",
    "            y_pred = model(X_batch.to(device))\n",
    "            loss = loss_fn(y_pred, y_batch.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().cpu()\n",
    "        losses[0].append(epoch_loss/batch_size)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader_val:\n",
    "                y_pred = model(X_batch.to(device))\n",
    "                loss += loss_fn(y_pred, y_batch.to(device))\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model = model.state_dict()\n",
    "            losses[1].append(loss.cpu()/batch_size)\n",
    "            print(f\"Epoch {epoch+1}: Train loss: {losses[0][epoch]:.2f} / Val loss: {losses[1][epoch]:.2f}\")\n",
    "    torch.save([best_model, tokenizer], \"language_model.pth\")\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GPU support\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "All there is to do now is use all previously created classes and functions to train the language model on Alice in Wonderland!\n",
    "\n",
    "**Your job**\n",
    "- Set the parameters\n",
    "    - sequence length\n",
    "    - number of epochs\n",
    "    - batch size\n",
    "    - tokeniser method\n",
    "- Initialise the variable\n",
    "    - Set the input text\n",
    "    - Initialise the tokeniser\n",
    "    - Transform the input text into features and targets\n",
    "    - Initialise the Language model\n",
    "    - Fit the model\n",
    "- Plot the result\n",
    "    - The plotting function is given after.\n",
    "    - Don't forget that the fit function returns a list of [loss train, loss validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = \n",
    "n_epochs = \n",
    "batch_size = \n",
    "tokenizer_choice =\n",
    "input = \n",
    "validation_ratio = \n",
    "\n",
    "# Initisalisation\n",
    "tokenizer = Tokenizer(input, tokenizer_choice)\n",
    "tokens = tokenizer.encode(input) \n",
    "features, targets = transform.transform_tokens(tokens, sequence_length)\n",
    "model = LanguageModel(tokenizer.n_vocab, sequence_length).to(device)\n",
    "train_slice = int(len(features)*(1-validation_ratio))\n",
    "train_set = (features[:train_slice], targets[:train_slice])\n",
    "val_set = (features[train_slice:], targets[train_slice:])\n",
    "\n",
    "# Training\n",
    "loss = fit(train_set, val_set, n_epochs, batch_size, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_losses(train_loss, val_loss):\n",
    "    plt.plot(train_loss, label='Training loss')\n",
    "    plt.plot(val_loss, label='validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"RNN training\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(loss[0],loss[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "The last step of this project!  \n",
    "Now that you have all parts working fine, and a **perfectly** trained neural network, you can start to predict some text with your language model.\n",
    "\n",
    "The predict function has already been written for you.   \n",
    "This example takes some part of the Alice in Wonderland text and asks the model to predict the next part out of its own predictions.  \n",
    "Of course, you can use your own text as input and experiment!   \n",
    "\n",
    "If the model is not giving satisfactory result: back to the modeling board!  \n",
    "\n",
    "**Try to get the best language model possible**  \n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_weights, tokenizer = torch.load(\"language_model.pth\", weights_only=False)\n",
    "model = LanguageModel(tokenizer.n_vocab, sequence_length).to(device)\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "# Generate a prompt\n",
    "input = raw_text[6991:10000]\n",
    "tokens = tokenizer.encode(input)\n",
    "tokens = tokens[0:model.sequence_length]\n",
    "\n",
    "def predict(tokens, tokenizer, transform, model):\n",
    "    model.eval()\n",
    "    print(tokenizer.method)\n",
    "    if(tokenizer.method in [Tokenisation_method.CHARACTER, Tokenisation_method.SUBWORD]):\n",
    "        token_splitter = ''\n",
    "    else:\n",
    "        token_splitter = ' '\n",
    "    print(f\"Prompt: [{tokenizer.decode(tokens)}]\\n\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(1000):\n",
    "            # format input array of int into PyTorch tensor\n",
    "            features, _ = transform.transform_tokens(tokens, model.sequence_length, with_target=False)\n",
    "            # generate logits as output from the model\n",
    "            prediction = model(features.to(device))\n",
    "            # convert logits into one character\n",
    "            index = [int(prediction.argmax())]\n",
    "            prediction\n",
    "            result = tokenizer.decode([idx for idx in index])\n",
    "            print(result, end=token_splitter)\n",
    "            # append the new character into the prompt for the next iteration\n",
    "            tokens.append(index[0])\n",
    "            tokens = tokens[1:]\n",
    "\n",
    "    \n",
    "predict(tokens ,tokenizer, transform, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS\n",
    "\n",
    "Now that you have a perfect understanding of the RNNs and its limitations, you can try your algorithm on other datasets!\n",
    "Below are some examples that you can get by using Huggingface datasets packages.  \n",
    "Each function returns a text string similar to this project's Alice in Wonderland input.  \n",
    "\n",
    "**AG News**  \n",
    "Description: Contains news articles categorized into four topics: World, Sports, Business, and Science/Technology.\n",
    "\n",
    "**WikiText-2 & WikiText-103**  \n",
    "Description: Extracted from Wikipedia, these datasets provide a large corpus for text prediction.\n",
    "\n",
    "**Amazon Review Popularity**  \n",
    "Description: Large-scale dataset of Amazon reviews labelled as positive or negative.\n",
    "\n",
    "**Yelp Review Polarity**  \n",
    "Description: Yelp reviews are categorised into positive and negative sentiments.\n",
    "\n",
    "\n",
    "**Your job**  \n",
    "\n",
    "- Train LSTM on one of these datasets.\n",
    "- Try to get the best possible model by varying the hyperparameters.\n",
    "- Try to vary the sequence length and mesure how it impacts the model.\n",
    "- Try to get the longest prediction without breaking the model.\n",
    "- Can the model predict prompts from other dataset?\n",
    "- Conclude   \n",
    "\n",
    "*Note*: If the trainnig takes too long, don't hesitate to train on smaller subset of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_ag_news():\n",
    "    \"\"\"Loads the AG News dataset and returns its text as a single string.\"\"\"\n",
    "    dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "    return \"\\n\".join([f\"{label} {text}\" for label, text in zip(dataset[\"label\"], dataset[\"text\"])])\n",
    "\n",
    "def load_wikitext2():\n",
    "    \"\"\"Loads the WikiText-2 dataset and returns its text as a single string.\"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    return \"\\n\".join(dataset[\"text\"])  \n",
    "\n",
    "def load_wikitext103():\n",
    "    \"\"\"Loads the WikiText-103 dataset and returns its text as a single string.\"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "    return \"\\n\".join(dataset[\"text\"])  \n",
    "\n",
    "def load_amazon_review():\n",
    "    \"\"\"Loads the Amazon Review Polarity dataset and returns its text as a single string.\"\"\"\n",
    "    dataset = load_dataset(\"amazon_polarity\", split=\"train\")\n",
    "    return \"\\n\".join([f\"{label} {title} {review}\" for label, title, review in zip(dataset[\"label\"], dataset[\"title\"], dataset[\"content\"])])\n",
    "\n",
    "def load_yelp_review():\n",
    "    \"\"\"Loads the Yelp Review Polarity dataset and returns its text as a single string.\"\"\"\n",
    "    dataset = load_dataset(\"yelp_polarity\", split=\"train\")\n",
    "    return \"\\n\".join([f\"{label} {text}\" for label, text in zip(dataset[\"label\"], dataset[\"text\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "\n",
    "print(\"\\nAG News Sample:\")\n",
    "print(load_ag_news()[:500])\n",
    "\n",
    "print(\"\\nWikiText-2 Sample:\")\n",
    "print(load_wikitext2()[:500])\n",
    "\n",
    "print(\"\\nWikiText-103 Sample:\")\n",
    "print(load_wikitext103()[:500])\n",
    "\n",
    "print(\"\\nAmazon Review Sample:\")\n",
    "print(load_amazon_review()[:500])\n",
    "\n",
    "print(\"\\nYelp Review Sample:\")\n",
    "print(load_yelp_review()[:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
