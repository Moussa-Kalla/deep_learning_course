{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "\n",
    "Welcome to the **Deep Learning** course! This course is designed to give you hands-on experience with the foundational concepts and advanced techniques in deep learning. You will explore:\n",
    "\n",
    "- Artificial Neural Networks and Gradient Descent\n",
    "- Convolutional Neural Networks (CNNs) for Computer Vision\n",
    "- Recurrent Neural Networks (RNNs) for Text Prediction\n",
    "- Diffusion Transformers for Image Generation\n",
    "\n",
    "Throughout the course, you'll engage in projects to solidify your understanding and gain practical skills in implementing deep learning algorithms.  \n",
    "\n",
    "Instructor: Dr. Adrien Dorise  \n",
    "Contact: adrien.dorise@hotmail.com  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Transformer Model for Text Prediction\n",
    "\n",
    "In this project, you will build a language model using a Transformer architecture in PyTorch. This exercise will guide you through the essential steps:\n",
    "\n",
    "**1. Dataset preparation**  \n",
    "\n",
    "- **Objective**: Convert raw text into numerical tokens suitable for a Transformer model.\n",
    "- **Steps**:\n",
    "   - Tokenization of the text\n",
    "   - Creation of batch sequences\n",
    "- **Note**:\n",
    "   - This ssection uses the same function that the one seen in the RNN project.\n",
    "   - Therefore, a script enompassing all these function is given seperately.\n",
    "\n",
    "**2. Building a Transformer Model with PyTorch**  \n",
    "\n",
    "- **Objective**: Undestand the parameters of a transformer model. The model is already written in the `model.py` file.\n",
    "- **Steps**:\n",
    "   - Analyse the given architecture\n",
    "   - Understand the hyperparameters in regards to the course\n",
    "   - Initialise the transformer model.  \n",
    "\n",
    "**3. Training the Transformer Model**  \n",
    "\n",
    "- **Objective**: Train the Transformer model using a suitable loss function and optimizer.\n",
    "- **Steps**:\n",
    "   - Defining a loss function for text prediction  \n",
    "   - Optimizing the model using gradient descent  \n",
    "   - Monitoring training progress and adjusting hyperparameters  \n",
    "\n",
    "**4. Generating Text with the Transformer Model**  \n",
    "\n",
    "- **Objective**: Use the trained model to generate new text sequences.\n",
    "- **Steps**:\n",
    "   - Implementing a prediction function for text generation  \n",
    "   - Using temperature scaling and top-k sampling to control randomness  \n",
    "   - Evaluating model output for coherence and fluency  \n",
    "\n",
    "**5. Compare with a pretrained transformer**  \n",
    "- **Objective**: Import a GPT-2 model and make prediction\n",
    "- **Steps**:\n",
    "   - Import a GPT-2 model with its tokenizer\n",
    "   - Tokenize an input prompt\n",
    "   - Analyse the output in regards to your own llms.  \n",
    "\n",
    "By completing this exercise, you will gain hands-on experience in implementing Transformers for text prediction while understanding key challenges like attention mechanisms, sequence modeling, and text generation strategies.\n",
    "\n",
    "*Credits*: This project is inspired by research on Transformer-based language models and builds upon concepts introduced in the \"Attention Is All You Need\" paper by Vaswani et al. (2017).  \n",
    "The transformer script was inspired by `https://medium.com/towards-data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preperation\n",
    "\n",
    "\n",
    "In this exercise, we will take the data from **Alice in Wonderland** by Lewis Caroll  \n",
    "The data can be found in the text file *alice_in_wonderland.txt* file in the same folder.  \n",
    "\n",
    "The book was dowloaded from this website:\n",
    "`https://www.gutenberg.org/ebooks/11`\n",
    "\n",
    "To simplify the prediction, all the dataset is set to lowercase.\n",
    "\n",
    "The preparation of the dataset is done similarly as the RNN project. Therefore, the function are written in another file called `data_preprocessing.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import Tokenizer, Transform_Tokens, Tokenisation_method\n",
    "\n",
    "# Dataset parameters\n",
    "sequence_length = 10\n",
    "validation_ratio = 0.1\n",
    "tokenization_method = Tokenisation_method.WORD\n",
    "\n",
    "\n",
    "# Import the txt file into a Python variable\n",
    "filename = \"alice_in_wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower() #Convert to lower case\n",
    "\n",
    "#Unlike RNN, transformers takes the whole input shifted by one as target\n",
    "tokenizer = Tokenizer(raw_text, tokenization_method)\n",
    "train_slice = int(len(raw_text)*(1-validation_ratio))\n",
    "tokens_train = tokenizer.encode(raw_text[:train_slice])\n",
    "tokens_val = tokenizer.encode(raw_text[train_slice:])\n",
    "transform = Transform_Tokens(tokens_train)\n",
    "features_train,targets_train = transform.transform_tokens(tokens_train,sequence_length)\n",
    "features_val,targets_val = transform.transform_tokens(tokens_val,sequence_length)\n",
    "train_set = (features_train, targets_train)\n",
    "val_set = (features_val, targets_val)\n",
    "\n",
    "\n",
    "print(f\"Unscaled torch Feature train -> Unscaled Torch Target train\")\n",
    "print(f\"Feature shape: {features_train.shape}\")\n",
    "print(f\"Target shape: {targets_train.shape}\")\n",
    "for i in range(5):\n",
    "    print(f\"{features_train[i]} -> {targets_train[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to focus on different parts of the input sequence simultaneously.\n",
    "In this project, you will compare a model created from scratch and the pre-trained GPT-3 model.\n",
    "\n",
    "<img src=\"../docs/transformers.jpg\" alt=\"transformers\" width=\"800\"/>  \n",
    "\n",
    "Below is a PyTorch implementation of multi-head attention. It contains all the blocks described in the course. Don't hesitate to review the figures in the course if you are lost.\n",
    "The steps are:\n",
    "\n",
    "- Implement the attention mechanism\n",
    "- Create the Linear layer\n",
    "- Implement positional encoding\n",
    "- Create the encoder part\n",
    "- Create the decoder part\n",
    "- Assemble the whole transformer\n",
    "    - Embedding vectors are created in the transformer class.\n",
    "\n",
    "As it is a consequent architecture, it was decided to split the module in a separate file.  \n",
    "Therefore, the whole transformer can be found in `model.py`\n",
    "\n",
    "Below is the training script to perform backpropagation on a transformer using PyTorch\n",
    "\n",
    "**Your Job:**\n",
    "- Understand how the transformer is implemented in PyTorch\n",
    "- Understand the training function\n",
    "- Initialise the model, by adjusting the hyperparameters accordingly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fit(train_set, val_set, n_epochs, batch_size, vocab_size, model, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    feat_train, targ_train = train_set\n",
    "    feat_val, targ_val = val_set\n",
    "    loader_train = data.DataLoader(data.TensorDataset(feat_train, targ_train), shuffle=False, batch_size=batch_size)\n",
    "    loader_val = data.DataLoader(data.TensorDataset(feat_val, targ_val), shuffle=False, batch_size=batch_size)\n",
    "    losses = [[],[]]\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in loader_train:\n",
    "            y_pred = model(X_batch.to(device), y_batch[:, :-1].to(device))\n",
    "            loss = loss_fn(y_pred.contiguous().view(-1, vocab_size), y_batch[:, 1:].contiguous().view(-1).to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().cpu()\n",
    "        losses[0].append(epoch_loss/batch_size)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader_val:\n",
    "                y_pred = model(X_batch.to(device), y_batch[:, :-1].to(device))\n",
    "                loss += loss_fn(y_pred.contiguous().view(-1, vocab_size), y_batch[:, 1:].contiguous().view(-1).to(device))\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model = model.state_dict()\n",
    "            losses[1].append(loss.cpu()/batch_size)\n",
    "            print(f\"Epoch {epoch+1}: Train loss: {losses[0][epoch]:.4f} / Val loss: {losses[1][epoch]:.4f}\")\n",
    "    torch.save([best_model, tokenizer], \"llm.pth\")\n",
    "    return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "\n",
    "#TODO: Ininitialise the transformer model\n",
    "\n",
    "# Model hyperparameters\n",
    "vocab_size = \n",
    "embedding_dim =\n",
    "n_attention_heads = \n",
    "n_layers = \n",
    "linear_dim = \n",
    "dropout = \n",
    "\n",
    "transformer = Transformer(vocab_size, vocab_size, embedding_dim, n_attention_heads, n_layers, linear_dim, sequence_length, dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that your dataset is ready, and your transformer is initialised, all is left to do is to train your own custom LLM!  \n",
    "The code snippets below will start the training of the model.  \n",
    "Note that depending on your input data, it can take a while, especially if you are not on GPU.  \n",
    "\n",
    "Don't hesitate to take only a subset of the whole text as input if it takes too long.\n",
    "\n",
    "**Your job**:\n",
    "- Train the LLM\n",
    "- Plot the loss curves\n",
    "- Make a prediction on an input prompt\n",
    "- Improve the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GPU support\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n",
    "transformer = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "# Training parameters\n",
    "batch_size = \n",
    "n_epochs = \n",
    "\n",
    "# Training\n",
    "loss = fit(train_set, val_set, n_epochs, batch_size, vocab_size, transformer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_losses(train_loss, val_loss):\n",
    "    plt.plot(train_loss, label='Training loss')\n",
    "    plt.plot(val_loss, label='validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"LLM training\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(loss[0],loss[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokens, tokenizer, model):\n",
    "    model.eval()\n",
    "    print(tokenizer.method)\n",
    "    if(tokenizer.method in [Tokenisation_method.CHARACTER, Tokenisation_method.SUBWORD]):\n",
    "        token_splitter = ''\n",
    "    else:\n",
    "        token_splitter = ' '\n",
    "    print(f\"Prompt: [{tokenizer.decode([int(tok) for tok in tokens[0]])}]\\n\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(500):\n",
    "            prediction = transformer(tokens.to(device), tokens.to(device))\n",
    "            prediction = prediction.view(-1,tokenizer.n_vocab).argmax(1)\n",
    "            result = tokenizer.decode([int(prediction[-1])])\n",
    "            print(result, end=token_splitter)\n",
    "\n",
    "            # append the new character into the prompt for the next iteration\n",
    "            tokens = torch.cat((tokens,prediction[-1:].unsqueeze(0).cpu()),1)\n",
    "            tokens = tokens[:,1:]\n",
    "\n",
    "\n",
    "predict(features_train[0:1,:] ,tokenizer, transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with pre-trained LLM\n",
    "\n",
    "Usually, you should have a correctly tuned model by now. It might not be perfect but don't forget that your model is only trained on a very small subset of data.  \n",
    "For instance, GPT-3 was trained on approximatively **410 billion** tokens with **175 billion parameters**. With a single GPU, it would take 355 years to train it from scratch.  \n",
    "\n",
    "In the flowllong code snippet, you have a demonstration of a GPT-2 implementation.  \n",
    "Feel free to experiment with it, and compare it to your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#TODO\n",
    "\n",
    "prompt = \n",
    "n_generated_tokens = \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=n_generated_tokens)\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "This is the last bonus section of this course!  \n",
    "In this one, you will have to compare the LSTM model and the transformer model.  \n",
    "Find a dataset you like, and try to train both models on it. Be careful about training time though.  \n",
    "\n",
    "Here are some comparison ideas:\n",
    "- Training time\n",
    "- Sequence length influence\n",
    "- Tokenizer influence\n",
    "- Correctness of answers\n",
    "- Versatility (is it able to answer never-seen prompts)\n",
    "- Robustness to overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END!\n",
    "\n",
    "Congratulations!  \n",
    "You have now completed this course about artificial intelligence! Â \n",
    "You should now have a good understanding of the basic principles of artificial intelligence, from the beginning in the 50's with the perceptron up to today with the transformer.  \n",
    "\n",
    "It is a fine knowledge basis on which you construct yourself. You are now well-prepared to tackle new challenges in Deep Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
